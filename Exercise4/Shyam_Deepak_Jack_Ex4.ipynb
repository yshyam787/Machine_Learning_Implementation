{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc713cda",
   "metadata": {
    "id": "bc713cda"
   },
   "source": [
    "# Machine Learning - Practical 4 - Deep Learning VS Trees\n",
    "\n",
    "\n",
    "Names: {YOUR NAMES}  \n",
    "Summer Term 2024   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e394c4e",
   "metadata": {
    "id": "0e394c4e"
   },
   "source": [
    "In this practical we are going to use a tabular dataset. We will test two different approaches - forests and neural networks and compare performance. We are also going to learn how to make trees interpretable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72933ee7",
   "metadata": {
    "id": "72933ee7"
   },
   "source": [
    "To prepare this tutorial we used [this paper](https://arxiv.org/pdf/2207.08815.pdf) with its [repository](https://github.com/LeoGrin/tabular-benchmark).\n",
    "\n",
    "For explained variance in trees, you can read more [here](https://scikit-learn.org/0.15/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218de310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218de310",
    "outputId": "93864191-1b7e-479f-bfd3-0759b4847266"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb0f7c1b730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "torch.manual_seed(42) # Set manual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2c837",
   "metadata": {
    "id": "3bb2c837"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "use_cuda = True\n",
    "use_cuda = False if not use_cuda else torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "torch.cuda.get_device_name(device) if use_cuda else 'cpu'\n",
    "print('Using device', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e24771e",
   "metadata": {
    "id": "8e24771e"
   },
   "source": [
    "## Load, clean and split the tabular dataset\n",
    "\n",
    "**Data description:**  \n",
    "Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.  \n",
    "\n",
    "Data is from https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "*Disclaimer* numbers below for Neural Networks, etc are outdated, so do not orient on these\n",
    "\n",
    "**Task:** - download the dataset in python and load it here. Check the dataset size and preliminary artifacts.  \n",
    "\n",
    "*Hint:*\n",
    "* How many unique values the target column should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebdb9f-26ff-4b24-8114-40346ab1dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - download the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d9f9e1",
   "metadata": {
    "id": "61d9f9e1"
   },
   "source": [
    "We use the preprocessing pipeline from [Grinsztajn, 2022](https://arxiv.org/pdf/2207.08815.pdf).\n",
    "\n",
    "**No missing data**    \n",
    "\n",
    "Remove all rows containing at least one missing entry.    \n",
    "\n",
    "*In practice people often do not remove rows with missing values but try to fill missing values in a column with the mean or median values for numerical data and mode or median values for categorical data. Sometimes even simple prediction models are used to fill in the gaps but we will remove rows or columns with missing values for the sake of simplicity. Sometimes, even the fact that data is missing could be data itself (think about patients who came or missed doctor appointment). In this case we are going with the most simple way to handle Nans - basically removing such entries.*\n",
    "\n",
    "**Balanced classes**   \n",
    "\n",
    "For classification, the target is binarised if there are multiple classes, by taking the two most numerous classes, and we keep half of samples in each class.\n",
    "\n",
    "**Low cardinality categorical features**   \n",
    "\n",
    "Remove categorical features with more than 40 items.\n",
    "\n",
    "**High cardinality numerical features**   \n",
    "\n",
    "Remove numerical features with less than 10 unique values. Convert numerical features with 2 unique values to categorical.\n",
    "\n",
    "**Encode categorical values**   \n",
    "\n",
    "Use label encodings for categorical variables\n",
    "\n",
    "*Hint:*\n",
    "* To make t easier to drop rows with nan values, merge $X$ and $Y$ in the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fAWW5kChD6MU",
   "metadata": {
    "id": "fAWW5kChD6MU"
   },
   "outputs": [],
   "source": [
    "target_column = 'income'\n",
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77435f",
   "metadata": {
    "id": "3d77435f"
   },
   "outputs": [],
   "source": [
    "def remove_nans(df):\n",
    "    '''\n",
    "    this fucntion removes rows with nans \n",
    "    '''\n",
    "    # TODO\n",
    "    return df\n",
    "\n",
    "\n",
    "def numerical_to_categorical(df, n=2, ignore=[target_column]):\n",
    "    '''\n",
    "    change the type of the column to categorical \n",
    "    if it has <= n unique values\n",
    "    '''\n",
    "    # TODO\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_columns_by_n(df, condition, n=10, direction='less', \n",
    "                        ignore=[target_column]):\n",
    "    '''\n",
    "   \n",
    "    Remove columns with more or less than n unique values. \n",
    "    Usually it makes sense to apply this function to columns with categorical values (see below where it is called).\n",
    "    With the default values we remove all numerical columns which have less than 10 unique values (except for the target_column).\n",
    "    '''\n",
    "    # TODO\n",
    "    return df\n",
    "\n",
    "def object_to_categorical(df):\n",
    "    '''\n",
    "    Make columns with the 'object' type categorical \n",
    "    and replace categories with label encodings\n",
    "    '''\n",
    "    # TODO\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033619f",
   "metadata": {
    "id": "9033619f"
   },
   "outputs": [],
   "source": [
    "df = dataset\n",
    "df = remove_nans(df)\n",
    "df = numerical_to_categorical(df, n=2, ignore=[target_column])\n",
    "\n",
    "df = remove_columns_by_n(df, n=10, direction='less', \n",
    "                         ignore=[target_column], condition=#TODO)\n",
    "df = object_to_categorical(df)\n",
    "df = remove_columns_by_n(df, n=40, direction='more', \n",
    "                         ignore=[target_column], condition=#TODO)\n",
    "assert not df.isna().any().any(), 'There are still nans in the dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22348559",
   "metadata": {
    "id": "22348559"
   },
   "outputs": [],
   "source": [
    "# TODO : make train-test split from the dataframe using the parameters above\n",
    "# expected results variable names - train_X, test_X, train_y, test_y\n",
    "\n",
    "X = \n",
    "Y = \n",
    "\n",
    "train_X, test_X, train_y, test_y = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77f002d6",
   "metadata": {
    "id": "77f002d6"
   },
   "source": [
    "**TODO :**  \n",
    "\n",
    "* Did you split the dataset in a stratified manner or not? Why?\n",
    "* How did the dataset dimensions change after preprocessing?\n",
    "* How many unique values are in the target variable? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309f4d6",
   "metadata": {
    "id": "f309f4d6"
   },
   "source": [
    "## Task 1: Create a GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cxbPa0evFrRF",
   "metadata": {
    "id": "cxbPa0evFrRF"
   },
   "outputs": [],
   "source": [
    "## TODO : define the GradientBoostingClassifier, \n",
    "## train it on the train set and predict on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YZ5j6BMKFrcJ",
   "metadata": {
    "id": "YZ5j6BMKFrcJ"
   },
   "outputs": [],
   "source": [
    "## TODO : print  accuracy, precision, recall\n",
    "## Hint : use functions from sklearn metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f0ffa",
   "metadata": {
    "id": "598f0ffa"
   },
   "outputs": [],
   "source": [
    "## TODO : Write a function which iterates over trees_amount, \n",
    "## train a classifier with a specified amount of trees and print accuracy, precision, and recall.\n",
    "## Note: the calculations may take several minutes (depending on the computer efficiency).\n",
    "\n",
    "def trees_amount_exploration(train_X, train_y, test_X, test_y, trees_amount=[1, 20, 50, 100]):\n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qcnxg4InEkRl",
   "metadata": {
    "id": "qcnxg4InEkRl"
   },
   "outputs": [],
   "source": [
    "trees_amount_exploration(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Write a function which iterates over the learning rate, \n",
    "## train a classifier with a specified amount of trees and print accuracy, precision, and recall.\n",
    "## Note: the calculations may take several minutes (depending on the computer efficiency).\n",
    "\n",
    "def learning_rate_exploration(train_X, train_y, test_X, test_y, learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5], trees_amount=100):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_exploration(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T-akqJfbEkVF",
   "metadata": {
    "id": "T-akqJfbEkVF"
   },
   "outputs": [],
   "source": [
    "## TODO : Write a function which iterates over different depths, \n",
    "## train a classifier with a specified depth and print accuracy, precision, and recall\n",
    "## Set trees_amount= 50 to make the calculations faster\n",
    "## Note: the calculations may take several minutes (depending on the computer efficiency).\n",
    "\n",
    "def max_depth_exploration(train_X, train_y, test_X, test_y, depths=[1,2,3,5]):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3oFAnFOnEkdX",
   "metadata": {
    "id": "3oFAnFOnEkdX"
   },
   "outputs": [],
   "source": [
    "max_depth_exploration(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75fc86d1",
   "metadata": {
    "id": "75fc86d1"
   },
   "source": [
    "**TODO :**   \n",
    "\n",
    "* How does the max_depth parameter influence the results? \n",
    "* How does the learning rate influence the results?\n",
    "* How does the number of trees in the ensemble influence the results?\n",
    "* Try to improve the accuracy by combining different max_depth, learning rate and number of trees. How well does your best model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c330e",
   "metadata": {
    "id": "aa0c330e"
   },
   "outputs": [],
   "source": [
    "## TODO -  sklearn trees have the attribute feature_importances_\n",
    "## make a plot, to show relative importance (maximum is 1) of your classifier and\n",
    "## order features from most relevant feature to the least relevant in the plot\n",
    "\n",
    "def plot_explained_variance(clf, X):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf90239",
   "metadata": {
    "id": "6cf90239"
   },
   "outputs": [],
   "source": [
    "## TODO : display the plot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e0872d8",
   "metadata": {
    "id": "2e0872d8"
   },
   "source": [
    "**TODO :** Interpret the plot.\n",
    "\n",
    "**TODO (optional):** Try to remove the least-important features and see what happens. Does to quality improve or degrade? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d40f1b",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Implement Tree from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000 # our implementation took ~1 min with this amount of samples, you can reduce the number if neccessary\n",
    "X_train, X_test, y_train, y_test = train_X.to_numpy()[:n_samples], test_X.to_numpy(), train_y.to_numpy()[:n_samples], test_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f74cc",
   "metadata": {},
   "source": [
    "Next, we will implement a simple decision tree classifier ourselves.\n",
    "We will use the Gini impurity as a criterion for splitting. It is defined for a set of labels as\n",
    "$$ G = \\sum_{i=0}^C p(i) * (1- p(i)) $$\n",
    "\n",
    "Given labels $l$ and split in $l_a$ and $l_b$, the weighted removed impurity can be computed by $G(l) - \\frac{|l_a|}{|l|}G(l_a) - \\frac{|l_b|}{|l|}G(l_b)$.\n",
    "\n",
    "Here is a simple explanation of the Gini impurity that you may find useful: https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "1. Implement a function `gini_impurity(y)` that computes the Gini impurity for an array of labels `y`.\n",
    "2. Implement a function `weighted_difference(y, split)`, that calculates the removed Gini impurity for a given boolean array `split` and `y`.\n",
    "3. Implement a function `find_best_split`, that performs an exhaustive search over all possible splits, i.e. for all features in x and all values of these features. \n",
    "\n",
    "Note: We have converted the training data to numpy above. Please use these arrays for the task. During debugging, it might be a good idea to reduce the number of data points via the n_samples argument above to speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the functions gini_impurity and weighted_difference\n",
    "\n",
    "def gini_impurity(y, labels=(0, 1)):\n",
    "    pass\n",
    "\n",
    "def weighted_difference(y, split, labels=(0, 1)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29844df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example application\n",
    "print(gini_impurity(y_train, labels=(0, 1)))\n",
    "\n",
    "split = X_train[:,0] < 40\n",
    "print(weighted_difference(y_train, split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06029fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function find_best_split\n",
    "def exhaustive_search(x, y):\n",
    "    pass\n",
    "\n",
    "def find_best_split(impurities_array, midpoints_array, features_array, verbose=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6042fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting functionality, works only if you have computed midpoints_array, impurities_array, features_array\n",
    "fig, ax = plt.subplots(4, 4, figsize=(14, 10), squeeze=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(14):\n",
    "    select_feature = features_array == i\n",
    "    impurities_feature = impurities_array[select_feature]\n",
    "    midpoints_feature = midpoints_array[select_feature]\n",
    "    \n",
    "    if i < len(ax):\n",
    "        ax[i].scatter(midpoints_feature, impurities_feature, s=4)\n",
    "        ax[i].set_title(f'Feature {i}')\n",
    "    ax[i].set_xlabel('Split Value')\n",
    "    ax[i].set_ylabel('Impurity Reduction')\n",
    "\n",
    "# Hide unused axes if any\n",
    "for j in range(i + 1, len(ax)):\n",
    "    ax[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b770a7",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "1. Implement a function `build_tree(X, t, depth)` which recursively builds a tree. Use the classes `Node` and `Leaf` as a data structure to build your tree.\n",
    "2. Implement a function `predict_tree(tree, x)` which makes a prediction for sample `x`. Obtain scores for the `wine` dataset and compare to `sklearn.tree.DecisionTree`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81924410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, n_feat, threshold):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_feat = n_feat\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef87849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function build_tree and predict_tree\n",
    "\n",
    "# Implement recursive tree function\n",
    "def build_tree(x, y, current_depth, max_depth=3, n_labels=2):\n",
    "    pass\n",
    "\n",
    "\n",
    "def predict_tree(node, x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ca58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree\n",
    "tree = build_tree(X_train, y_train, current_depth=0, max_depth=3, n_labels=2)\n",
    "predictions = predict_tree(tree, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test scores\n",
    "print('Accuracy Training: ', accuracy_score(y_train, predict_tree(tree, X_train)))\n",
    "print('Accuracy: ', accuracy_score(test_y, predictions))\n",
    "print('Precision: ', precision_score(test_y, predictions, average='macro'))\n",
    "print('Recall: ', recall_score(test_y, predictions, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "jhE75y_UN7Qx",
   "metadata": {
    "id": "jhE75y_UN7Qx"
   },
   "source": [
    "## Prepare for deep learning\n",
    "### Add all the necessary training functions \n",
    "*You can reuse them from previous practical exercises*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LfDLPO2QODES",
   "metadata": {
    "id": "LfDLPO2QODES"
   },
   "outputs": [],
   "source": [
    "## TODO write a function that calculates the accuracy\n",
    "## Hint - you can use yours from practical 3 \n",
    "\n",
    "def accuracy(correct, total): \n",
    "    \"\"\"\n",
    "    function to calculate the accuracy given the\n",
    "        correct: number of correctly classified samples\n",
    "        total: total number of samples\n",
    "    returns the ratio\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EVe7SFnrODHP",
   "metadata": {
    "id": "EVe7SFnrODHP"
   },
   "outputs": [],
   "source": [
    "## TODO : Define a train and validation functions here\n",
    "## Hint - you can use yours from practical 3 \n",
    "\n",
    "def train(dataloader, optimizer, model, loss_fn, device, master_bar):\n",
    "    \"\"\" method to train the model \"\"\"\n",
    "    # Todo\n",
    "\n",
    "\n",
    "def validate(dataloader, model, loss_fn, device, master_bar):\n",
    "    \"\"\" method to compute the metrics on the validation set \"\"\"\n",
    "    # Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pzGM0bPqxw3V",
   "metadata": {
    "id": "pzGM0bPqxw3V"
   },
   "outputs": [],
   "source": [
    "#TODO write a run_training function that \n",
    "# - calls the train and validate functions for each epoch\n",
    "# - saves the train_losses, val_losses, train_accs, val_accs as arrays for each epoch\n",
    "## Hint - you can use yours from practical 3 \n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def run_training(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader):\n",
    "    \"\"\" method to run the training procedure \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ngflrAEJxw5t",
   "metadata": {
    "id": "ngflrAEJxw5t"
   },
   "outputs": [],
   "source": [
    "# TODO write a plot_model_progress function \n",
    "## It should plot epochs vs metric progress \n",
    "## Hint - you can use yours from practical 2 or 3 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9iqaRi2xRADg",
   "metadata": {
    "id": "9iqaRi2xRADg"
   },
   "source": [
    "### Convert a pandas dataframe to a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcbc2c",
   "metadata": {
    "id": "e3dcbc2c"
   },
   "outputs": [],
   "source": [
    "## TODO : Define the dataset, apply normalization in the getitem method\n",
    "## Hint : you can use/adapt your code from practical 2\n",
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_x, df_y, mean=None, std=None, normalise=True):\n",
    "        '''\n",
    "        TODO: save params to self attributes, \n",
    "        x is data without target column\n",
    "        y is target column\n",
    "        transform df to_numpy\n",
    "        ''' \n",
    "        self.x = \n",
    "        self.y = \n",
    "        self.mean = \n",
    "        self.std = \n",
    "        self.normalise = \n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO: return the length of the whole dataset\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## TODO: return X, y by index, normalized if needed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3jJXxpF-KIpz",
   "metadata": {
    "id": "3jJXxpF-KIpz"
   },
   "outputs": [],
   "source": [
    "## TODO : calculate mean and std for the train set\n",
    "## Hint : be careful with categorical values. Convert them them to numerical \n",
    "## Hint : the response variable should be of datatype integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ef6dNkihxzYd",
   "metadata": {
    "id": "Ef6dNkihxzYd"
   },
   "outputs": [],
   "source": [
    "# TODO : define new datasets with mean, std and normalise=True\n",
    "# be careful with the labels, they should start from 0!\n",
    "\n",
    "\n",
    "## TODO : define dataloaders, with specified batch size and shuffled\n",
    "batch_size = 256\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "N28MlXQ7ON1j",
   "metadata": {
    "id": "N28MlXQ7ON1j"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NT-EtgZKOLb0",
   "metadata": {
    "id": "NT-EtgZKOLb0"
   },
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The logistic regression model inherits from torch.nn.Module \n",
    "    which is the base class for all neural network modules.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Initializes internal Module state. \"\"\"\n",
    "        super(LR, self).__init__()\n",
    "        # TODO define linear layer for the model\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the computation performed at every call. \"\"\"\n",
    "        # What are the dimensions of your input layer?\n",
    "        x = x.to(torch.float32)\n",
    "        # TODO run the data through the layer\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eWDd_hYGOZSj",
   "metadata": {
    "id": "eWDd_hYGOZSj"
   },
   "outputs": [],
   "source": [
    "## TODO define model, loss and optimizers\n",
    "## don't forget to move everything for the correct devices\n",
    "## \n",
    "lr=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6EFwKn-iOgNv",
   "metadata": {
    "id": "6EFwKn-iOgNv"
   },
   "outputs": [],
   "source": [
    "## TODO train the network\n",
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H-IJWOsATYul",
   "metadata": {
    "id": "H-IJWOsATYul"
   },
   "outputs": [],
   "source": [
    "## todo - plot epochs vs loss with plot_model_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HjAaYdd8TZed",
   "metadata": {
    "id": "HjAaYdd8TZed"
   },
   "outputs": [],
   "source": [
    "## todo - plot epochs and accuracy with plot_model_progress\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb1bca9a",
   "metadata": {
    "id": "cb1bca9a"
   },
   "source": [
    "\n",
    "## Create a simple MLP\n",
    "\n",
    "As the default tree has 3 layers, let's make a MLP with 3 linear layers and ReLU.\n",
    "Please notice that making convolutions on tabular data does not make much sense even though it is technically possible.   \n",
    "\n",
    "**TODO :** Explain why making convolutions on tabular data does not make much sense. Why do we use an MLP, not a CNN from the previous homework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecbb52",
   "metadata": {
    "id": "4eecbb52"
   },
   "outputs": [],
   "source": [
    "class TabularNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Initializes internal Module state. \"\"\"\n",
    "        super(TabularNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # TODO : define 3 linear layer with sizes \n",
    "            # input_dim -> input_dim // 2 -> output_dim\n",
    "            # using ReLU as nonlinearity\n",
    "            \n",
    "        )\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the computation performed at every call. \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5OGid8SpMvZz",
   "metadata": {
    "id": "5OGid8SpMvZz"
   },
   "outputs": [],
   "source": [
    "## TODO : define model, optimizer, cross entropy loss,\n",
    "## put model to the device, and train mode\n",
    "## you can optionally try to add regularization \n",
    "lr=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z-WNxtczM5Iv",
   "metadata": {
    "id": "Z-WNxtczM5Iv"
   },
   "outputs": [],
   "source": [
    "## TODO : Train model\n",
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I_xVns4qM5eO",
   "metadata": {
    "id": "I_xVns4qM5eO"
   },
   "outputs": [],
   "source": [
    "## todo - plot epochs vs loss with plot_model_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JRAEaYnYUpd_",
   "metadata": {
    "id": "JRAEaYnYUpd_"
   },
   "outputs": [],
   "source": [
    "## todo - plot epochs and accuracy with plot_model_progress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "J8HbVqiUznBu",
   "metadata": {
    "id": "J8HbVqiUznBu"
   },
   "source": [
    "**TODO:** Did your network perform better or worse than the GradientBoostingClassifier on this dataset? Why? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc0f5f87",
   "metadata": {
    "id": "cc0f5f87"
   },
   "source": [
    "## Bonus tasks (optional)\n",
    "* Try to use SGD instead of Adam as optimizer. What do you notice?\n",
    "Here are different opinions on this topic:\n",
    "  * https://codeahoy.com/questions/ds-interview/33/#:~:text=Adam%20tends%20to%20converge%20faster,converges%20to%20more%20optimal%20solutions.\n",
    "  * https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/ \n",
    "  * https://datascience.stackexchange.com/questions/30344/why-not-always-use-the-adam-optimization-technique\n",
    "\n",
    "* Try to make your MLP twice deeper. What do you notice? Why?\n",
    "\n",
    "## Advanced topic to read about:\n",
    "**Tools which may be helpful for data exploration:**\n",
    "* df.describe() - returns some basic statistics for your dataset - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n",
    "* ydata-profiling (previous pandas-profiling) - generates interactive data exploration report: basic statistics, nans, correlations between different features - https://github.com/ydataai/ydata-profiling\n",
    "\n",
    "**Tree libraries**\n",
    "* XGBoost - XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n",
    "* LightGBM - industrial library for XGBoost from Miscrosoft. LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient. https://lightgbm.readthedocs.io/en/v3.3.2/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
